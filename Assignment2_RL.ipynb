{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "In the above model for CartPole, try to modify the model to experiment new ideas. \n",
    "\n",
    "Requirements:\n",
    "1. Use cross-entropy method. \n",
    "2. Change NN to have more or less neurons, more layers, different activation functions, different optimizer, learning rate. How does it impact the results.\n",
    "3. Change BATCH_SIZE and PERCENTILE. How does it impact the results.\n",
    "4. New rule in this game: you must run your same action twice in every step before the agent receives the observation and chooses next action. Modify and train your model. Show the results and discuss the difference.\n",
    "5. Extend from above. Build another NN to use one observation to generate two consecutive actions. You will have 4 possible outputs: LL, LR, RL, RR. Then you will apply the two consecutive actions in the environment and then receive the last observation to predict next two consecutive actions. Train the model and show your result. \n",
    "\n",
    "Your grade is based on the completeness. \n",
    "\n",
    "Submit in BOTH ipynb and html formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\srikr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\srikr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m model_default\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer_default, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# 2. Train the model with cross-entropy method\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_default\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsecutive_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# 3. Change NN architecture, activation, optimizer, and learning rate\u001b[39;00m\n\u001b[0;32m     97\u001b[0m model_custom \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     98\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,)),\n\u001b[0;32m     99\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    100\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m2\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    101\u001b[0m ])\n",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, env, num_iterations, batch_size, percentile, consecutive_actions)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, env, num_iterations, batch_size, percentile, consecutive_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m---> 32\u001b[0m         rewards \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsecutive_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m         elite_states, elite_actions \u001b[38;5;241m=\u001b[39m select_elites(rewards, percentile, consecutive_actions)\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Flatten elite_states to fit the input shape\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m, in \u001b[0;36mrun_episodes\u001b[1;34m(model, env, num_episodes, consecutive_actions)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Repeat the same action for 'consecutive_actions' times\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(consecutive_actions):\n\u001b[1;32m---> 20\u001b[0m         action_prob \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     21\u001b[0m         action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;241m2\u001b[39m, p\u001b[38;5;241m=\u001b[39maction_prob)\n\u001b[0;32m     22\u001b[0m         state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\shape_base.py:591\u001b[0m, in \u001b[0;36mexpand_dims\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m    589\u001b[0m     a \u001b[38;5;241m=\u001b[39m asarray(a)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(axis) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    594\u001b[0m     axis \u001b[38;5;241m=\u001b[39m (axis,)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Function to run episodes and get total rewards\n",
    "def run_episodes(model, env, num_episodes, consecutive_actions=1):\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Repeat the same action for 'consecutive_actions' times\n",
    "            for _ in range(consecutive_actions):\n",
    "                action_prob = model.predict(np.expand_dims(state, axis=0))[0]\n",
    "                action = np.random.choice(2, p=action_prob)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    return total_rewards\n",
    "\n",
    "# Function to train the model using the cross-entropy method\n",
    "def train_model(model, env, num_iterations, batch_size, percentile, consecutive_actions=1):\n",
    "    for iteration in range(num_iterations):\n",
    "        rewards = run_episodes(model, env, batch_size, consecutive_actions)\n",
    "        elite_states, elite_actions = select_elites(rewards, percentile, consecutive_actions)\n",
    "\n",
    "        # Flatten elite_states to fit the input shape\n",
    "        elite_states_flat = np.reshape(elite_states, (elite_states.shape[0] * elite_states.shape[1], elite_states.shape[2]))\n",
    "\n",
    "        # One-hot encode elite_actions\n",
    "        elite_actions_one_hot = tf.keras.utils.to_categorical(elite_actions, num_classes=2)\n",
    "\n",
    "        # Train the model on elite_states and elite_actions\n",
    "        model.fit(elite_states_flat, elite_actions_one_hot, epochs=1, verbose=0)\n",
    "\n",
    "        # Evaluate the model every 10 iterations\n",
    "        if iteration % 10 == 0:\n",
    "            total_rewards = run_episodes(model, env, 100, consecutive_actions)\n",
    "            mean_reward = np.mean(total_rewards)\n",
    "            print(f\"Iteration: {iteration}, Mean Reward: {mean_reward}\")\n",
    "\n",
    "# Function to select elites based on rewards\n",
    "def select_elites(rewards, percentile, consecutive_actions):\n",
    "    reward_threshold = np.percentile(rewards, percentile)\n",
    "\n",
    "    elite_indices = np.where(rewards >= reward_threshold)[0]\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "\n",
    "    for idx in elite_indices:\n",
    "        state, action = generate_elite_sequence(idx, consecutive_actions)\n",
    "        elite_states.append(state)\n",
    "        elite_actions.append(action)\n",
    "\n",
    "    return np.array(elite_states), np.array(elite_actions)\n",
    "\n",
    "# Function to generate elite sequence for a given episode index\n",
    "def generate_elite_sequence(episode_index, consecutive_actions):\n",
    "    state = env.reset()\n",
    "    elite_states = [state]\n",
    "    elite_actions = []\n",
    "\n",
    "    for _ in range(consecutive_actions):\n",
    "        action_prob = model.predict(np.expand_dims(state, axis=0))[0]\n",
    "        action = np.random.choice(2, p=action_prob)\n",
    "        state, _, _, _ = env.step(action)\n",
    "\n",
    "        elite_states.append(state)\n",
    "        elite_actions.append(action)\n",
    "\n",
    "    return np.array(elite_states), np.array(elite_actions)\n",
    "\n",
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 1. Default Model\n",
    "model_default = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "optimizer_default = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_default.compile(optimizer=optimizer_default, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Train the model with cross-entropy method\n",
    "train_model(model_default, env, num_iterations=100, batch_size=100, percentile=70, consecutive_actions=1)\n",
    "\n",
    "# 3. Change NN architecture, activation, optimizer, and learning rate\n",
    "model_custom = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='tanh', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "optimizer_custom = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model_custom.compile(optimizer=optimizer_custom, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Train the custom model with cross-entropy method\n",
    "train_model(model_custom, env, num_iterations=100, batch_size=100, percentile=70, consecutive_actions=1)\n",
    "\n",
    "# 5. New rule: Run the same action twice in every step\n",
    "# 6. Train the model with the new rule (consecutive_actions=2)\n",
    "train_model(model_custom, env, num_iterations=100, batch_size=100, percentile=70, consecutive_actions=2)\n",
    "\n",
    "# 7. Extend the model for two consecutive actions (LL, LR, RL, RR)\n",
    "model_extended = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='tanh', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "optimizer_extended = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model_extended.compile(optimizer=optimizer_extended, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 8. Train the extended model\n",
    "train_model(model_extended, env, num_iterations=100, batch_size=100, percentile=70, consecutive_actions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
